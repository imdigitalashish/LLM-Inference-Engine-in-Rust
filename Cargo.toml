[package]
name = "llm-inference-engine"
version = "0.1.0"
edition = "2021"
description = "Production-grade LLM inference engine in Rust"
license = "MIT"

[dependencies]
# Candle ML framework
candle-core = "0.9"
candle-nn = "0.9"
candle-transformers = "0.9"

# HuggingFace Hub for model downloads
hf-hub = { version = "0.4", features = ["tokio"] }

# Tokenizers
tokenizers = "0.21"

# Async runtime
tokio = { version = "1.43", features = ["full"] }

# Web framework for API server
axum = { version = "0.8", features = ["tokio"] }
tower = "0.5"
tower-http = { version = "0.6", features = ["cors", "trace"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error handling
anyhow = "1.0"
thiserror = "2.0"

# Logging and tracing
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# CLI
clap = { version = "4.5", features = ["derive", "env"] }

# Utilities
rand = "0.8"
futures = "0.3"
tokio-stream = "0.1"
uuid = { version = "1.11", features = ["v4"] }

[features]
default = []
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
accelerate = ["candle-core/accelerate", "candle-nn/accelerate", "candle-transformers/accelerate"]
mkl = ["candle-core/mkl", "candle-nn/mkl", "candle-transformers/mkl"]

[[bin]]
name = "llm-engine"
path = "src/main.rs"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
